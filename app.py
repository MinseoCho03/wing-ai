# app.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field, ConfigDict
from typing import Any, Dict, List, Optional
import time
import torch
import numpy as np
import os
import re

from wing_ai.pipeline import WINGAIPipeline

# ---------------------------------------------------------
# í™˜ê²½ ë³€ìˆ˜ ì„¸íŒ… (Dockerfileê³¼ ì¼ì¹˜)
# ---------------------------------------------------------
os.environ.setdefault("HF_HOME", "/opt/hf-cache")
os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")
os.environ.setdefault("HF_HUB_OFFLINE", "1")

app = FastAPI(title="WING AI API", version="0.1.6")

# ---------------------------------------------------------
# Globals
# ---------------------------------------------------------
pipeline: Optional[WINGAIPipeline] = None
_ready: bool = False

# ---------------------------------------------------------
# Helpers
# ---------------------------------------------------------
def _normalize_text(s: Optional[str]) -> str:
    if not s:
        return ""
    # í•„ìš”ì‹œ ì¶”ê°€ ì •ê·œí™”(ëŒ€ì†Œë¬¸ì, ê³µë°± ë“±)
    return s.strip()

def _denamespace_kw(raw_q: str, main_keyword: Optional[str]) -> str:
    """
    'ì—”ë¹„ë””ì•„ ì´ì¬ìš©'ì²˜ëŸ¼ ë©”ì¸í‚¤ì›Œë“œê°€ query ì ‘ë‘ë¡œ ë¶™ì€ ì¼€ì´ìŠ¤ë¥¼ ì •ê·œí™”í•˜ì—¬
    ì„œë¸Œí‚¤ì›Œë“œë§Œ ë‚¨ê¸°ê±°ë‚˜, ë©”ì¸í‚¤ì›Œë“œ ìì²´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    q = _normalize_text(raw_q)
    if not main_keyword:
        return q

    mk = _normalize_text(main_keyword)
    if q == mk:
        return q

    # ì ‘ë‘ ì œê±° íŒ¨í„´ë“¤: "ë©”ì¸í‚¤ì›Œë“œ " / "ë©”ì¸í‚¤ì›Œë“œ-" / "ë©”ì¸í‚¤ì›Œë“œ_" / "ë©”ì¸í‚¤ì›Œë“œ:"
    patterns = [
        rf"^{re.escape(mk)}\s+",
        rf"^{re.escape(mk)}[-_:/|]\s*",
    ]
    for pat in patterns:
        q2 = re.sub(pat, "", q)
        if q2 != q:
            return q2.strip()

    return q

def _article_contains_main(art: Dict[str, Any], main_keyword: str) -> bool:
    if not main_keyword:
        return False
    t = (_normalize_text(art.get("title")) + " " + _normalize_text(art.get("description"))).strip()
    if not t:
        return False
    # í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸° ê¸°ë°˜ í¬í•¨ìœ¼ë¡œë„ ì¶©ë¶„íˆ ë³´ìˆ˜ì . í•„ìš” ì‹œ í˜•íƒœì†Œ/ì •ê·œì‹ ê²½ê³„ ê°•í™” ê°€ëŠ¥.
    return main_keyword in t

def _build_articles_by_keyword(
    results_list: List[Dict],
    main_keyword: Optional[str],
) -> Dict[str, List[Dict[str, Any]]]:
    """
    í¬ë¡¤ë§ ê²°ê³¼ ë¸”ë¡ì„ {ì •ê·œí™”ëœ_í‚¤ì›Œë“œ: [ê¸°ì‚¬ë“¤]} í˜•íƒœë¡œ ë³€í™˜.
    - queryì—ì„œ ë©”ì¸í‚¤ì›Œë“œ ì ‘ë‘ë¥¼ ì œê±°(ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±°)
    - ë©”ì¸í‚¤ì›Œë“œ ë…¸ë“œëŠ” ê¸°ì‚¬ ìœ ë¬´ì™€ ë¬´ê´€í•˜ê²Œ ë°˜ë“œì‹œ ìƒì„±
    - ğŸ”¥ í•˜ì´ë“œë ˆì´ì…˜: ì„œë¸Œ ë²„í‚· ê¸°ì‚¬ ì¤‘ ì œëª©/ìš”ì•½ì— ë©”ì¸í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ë¥¼ ë©”ì¸ ë²„í‚·ì—ë„ í•¨ê»˜ ë‹´ì•„ co-occurrence ë³´ì¥
    """
    out: Dict[str, List[Dict[str, Any]]] = {}

    # 1) ê¸°ë³¸ ë¹Œë“œ (+ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±°)
    for block in results_list:
        raw_q = block.get("query")
        if not raw_q:
            continue
        kw = _denamespace_kw(raw_q, main_keyword)
        items = block.get("items", []) or []
        bucket = out.setdefault(kw, [])
        for it in items:
            bucket.append({
                "title": it.get("title"),
                "description": it.get("description"),
                "link": it.get("link"),
                "originallink": it.get("originallink"),
                "pubDate": it.get("pubDate"),
            })

    # 2) ë©”ì¸ ë…¸ë“œ ë³´ì¥
    if main_keyword:
        out.setdefault(main_keyword, [])

        # 3) ğŸ”¥ ë©”ì¸ ë²„í‚· í•˜ì´ë“œë ˆì´ì…˜
        main_bucket = out[main_keyword]
        seen = {a.get("link") for a in main_bucket if isinstance(a, dict)}
        for kw, bucket in out.items():
            if kw == main_keyword:
                continue
            for art in bucket:
                if not isinstance(art, dict):
                    continue
                lk = art.get("link")
                if lk in seen:
                    continue
                if _article_contains_main(art, main_keyword):
                    main_bucket.append(art)
                    seen.add(lk)

    return out


def _to_py(obj):
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, dict):
        return {k: _to_py(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_to_py(x) for x in obj]
    if isinstance(obj, tuple):
        return tuple(_to_py(x) for x in obj)
    return obj


# ---------------------------------------------------------
# Schemas
# ---------------------------------------------------------
class CrawlingItem(BaseModel):
    model_config = ConfigDict(extra='ignore')
    link: Optional[str] = None
    title: Optional[str] = None
    pubDate: Optional[str] = None
    originallink: Optional[str] = None
    description: Optional[str] = None


class CrawlingResultBlock(BaseModel):
    model_config = ConfigDict(extra='ignore')
    query: str
    need: Optional[int] = None
    collectedCount: Optional[int] = None
    totalEstimated: Optional[int] = None
    items: List[CrawlingItem] = Field(default_factory=list)
    done: Optional[bool] = None
    nextStartHint: Optional[str] = None


class CrawlingPayload(BaseModel):
    model_config = ConfigDict(extra='ignore')
    mainKeyword: Optional[str] = None
    subKeywords: Optional[List[str]] = None
    queryCount: Optional[int] = None
    results: List[CrawlingResultBlock]


class GraphNode(BaseModel):
    model_config = ConfigDict(extra='allow')
    id: str
    importance: float


class GraphEdge(BaseModel):
    model_config = ConfigDict(extra='allow')
    source: str
    target: str
    weight: Optional[float] = None
    cooccurrence: Optional[float] = None
    similarity: Optional[float] = None
    articles: Optional[List[Dict[str, Any]]] = None
    sentiment_score: Optional[float] = None
    sentiment_label: Optional[str] = None
    sentiment_subject: Optional[str] = None
    sentiment_derivation: Optional[str] = None
    hops_to_main: Optional[int] = None


class GraphMetadata(BaseModel):
    model_config = ConfigDict(extra='allow')
    total_nodes: int
    total_edges: int
    processing_time: Dict[str, float]


class GraphResponse(BaseModel):
    model_config = ConfigDict(extra='allow')
    nodes: List[GraphNode]
    edges: List[GraphEdge]
    metadata: GraphMetadata


# ---------------------------------------------------------
# Lifecycles
# ---------------------------------------------------------
@app.on_event("startup")
def load_pipeline():
    global pipeline, _ready
    boot_t0 = time.time()
    try:
        pipeline = WINGAIPipeline(config_path="config.yaml")

        # ëª¨ë¸ ë¡œë“œ/ì›œì—… (ì´ë¯¸ ìºì‹œê°€ bake-in ë˜ì–´ìˆì–´ ë§¤ìš° ë¹ ë¦„)
        pipeline._ensure_embedding_model_loaded()
        pipeline._ensure_sentiment_model_loaded()

        with torch.inference_mode():
            _ = pipeline.embedding_model.encode(["warmup"], convert_to_tensor=False)
            sa = pipeline.sentiment_analyzer
            inputs = sa.tokenizer("warmup", return_tensors="pt", truncation=True, max_length=32)
            _ = sa.model(**inputs)

        _ready = True
        print(f"[startup] âœ… Warm-up complete in {(time.time()-boot_t0):.2f}s")
    except Exception as e:
        _ready = False
        print(f"[startup] âš ï¸ Warm-up failed: {e}")


# ---------------------------------------------------------
# Health / Status
# ---------------------------------------------------------
@app.get("/")
def root():
    return {"service": "wing-ai", "version": app.version, "status": "ok" if _ready else "warming"}


@app.get("/health")
def health():
    return {"status": "ok" if _ready else "warming"}


# ---------------------------------------------------------
# Main Endpoint
# ---------------------------------------------------------
@app.post("/process", response_model=GraphResponse)
def process_news(payload: CrawlingPayload, mode: str = "investment"):
    if pipeline is None or not _ready:
        raise HTTPException(status_code=503, detail="Pipeline not ready")

    total_t0 = time.time()

    # 1) ì…ë ¥ ì •ë¦¬
    prep_t0 = time.time()
    results_list = [r.model_dump() for r in payload.results]
    # âš ï¸ ë©”ì¸í‚¤ì›Œë“œ ì „ë‹¬í•˜ì—¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±° + í•˜ì´ë“œë ˆì´ì…˜ ìˆ˜í–‰
    articles_by_kw = _build_articles_by_keyword(results_list, main_keyword=payload.mainKeyword)
    prep_ms = (time.time() - prep_t0) * 1000.0

    # 2) íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipe_t0 = time.time()
    result: Dict[str, Any] = pipeline.process(
        articles_by_kw,
        mode=mode,
        main_keyword=payload.mainKeyword
    )
    pipe_ms = (time.time() - pipe_t0) * 1000.0

    # 3) ì‘ë‹µ ì§ë ¬í™”
    resp_t0 = time.time()
    nodes_list: List[GraphNode] = [
        GraphNode(id=str(kw), importance=float(imp))
        for kw, imp in result.get("nodes", {}).items()
    ]
    edges_list: List[GraphEdge] = []
    for (src, tgt), data in result.get("edges", {}).items():
        edge_payload: Dict[str, Any] = {"source": src, "target": tgt}
        edge_payload.update(_to_py(data))
        # ë¶ˆí•„ìš”/ì¤‘ë³µ ê°ì„± í•„ë“œ ì •ë¦¬
        articles = edge_payload.get("articles")
        if isinstance(articles, list):
            for art in articles:
                if isinstance(art, dict):
                    art.pop("sentiment", None)
                    art.pop("sentiment_score", None)
        edges_list.append(GraphEdge(**edge_payload))

    resp_ms = (time.time() - resp_t0) * 1000.0
    total_ms = (time.time() - total_t0) * 1000.0

    meta = GraphMetadata(
        total_nodes=len(nodes_list),
        total_edges=len(edges_list),
        processing_time={
            "total_ms": round(total_ms, 2),
            "preparation_ms": round(prep_ms, 2),
            "pipeline_ms": round(pipe_ms, 2),
            "response_ms": round(resp_ms, 2),
        }
    )

    return GraphResponse(nodes=nodes_list, edges=edges_list, metadata=meta)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=8080, reload=False)
